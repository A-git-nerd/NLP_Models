# Comparative Analysis: All Questions
## Overview
This report presents a comprehensive comparison of all models evaluated across four different NLP tasks involving Urdu language processing.

## Question 1: Sentiment Analysis (Classification Task)

### Performance Summary
| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| RNN   | 0.531    | 0.514     | 0.792  | 0.623    |
| GRU   | 0.551    | 0.554     | 0.425  | 0.481    |
| LSTM  | 0.567    | 0.592     | 0.375  | 0.459    |
| BiLSTM| 0.571    | 0.555     | 0.633  | 0.591    |
| mBERT | 0.678    | 0.847     | 0.417  | 0.559    |
|XLM-RoBERTa|0.714 | 0.705     | 0.717  | 0.711    |

**Winner**: **XLM-RoBERTa** - Best overall performance with balanced precision-recall.

## Question 2: Embedding Comparison for Sentiment Analysis
### Performance Summary

| Model                        | Accuracy | Precision | Recall | F-Score |
|-------                       |----------|-----------|--------|---------|
| LSTM (without embeddings)    | 0.584    | 0.580     | 0.542  | 0.560   |
| LSTM with Word2Vec           | 0.478    | 0.441     | 0.250  | 0.319   |
| LSTM with GloVe              | 0.490    | 0.490     | 1.000  | 0.658   |
| LSTM with FastText           | 0.522    | 1.000     | 0.025  | 0.049   |
| LSTM with ELMo               | 0.543    | 0.643     | 0.150  | 0.243   |

**Winner**: **Random Embeddings** - English pre-trained embeddings fail on Urdu text.

## Question 3: Machine Translation (English â†’ Urdu)
### Performance Summary

| Model               | BLEU Score |
|-------              |------------|
| RNN Seq2Seq         | 0.00       |
| BiRNN Seq2Seq       | 18.996     |
| LSTM Seq2Seq        | 12.703     |
| Transformer         | 18.996     |
| mBART-50 (zero-shot)| N/A        |

BLEU calculation error due to tokenization, but qualitative analysis shows best translations.
**Winner**: **mBART-50** - Only model producing usable translations.

## Question 4: Embedding Impact on Translation
### Performance Summary

| Model             | BLEU Score | Final Val Loss | Training Time (s) | Avg Time/Epoch (s) |
|-------            |------------|----------------|-------------------|------------------- |
| Random Embeddings | 18.996     | 5.390          | 60.25             | 0.301              |
| GloVe Embeddings  | 18.996     | 5.475          | 53.89             | 0.269              |

**Winner**: **Tie** - Both fail equally, GloVe provides no benefit.

## Cross-Question Insights
### 1. Pre-trained Models Dominate
**Finding**: Pre-trained multilingual models (XLM-RoBERTa, mBERT, mBART) significantly outperform custom-trained models.
**Reason**: Transfer learning from massive multilingual data overcomes Urdu's low-resource challenges.

### 2. Language-Specific Embeddings Critical
**Finding**: English embeddings (Word2Vec, GloVe, FastText) hurt Urdu performance.
**Q2 Evidence**:
- Random embeddings: 0.560 F-score
- Word2Vec (English): 0.319 F-score (-43%)
- GloVe (English): Severe prediction bias
**Q4 Evidence**:
- GloVe provides no benefit for translation
- Same poor performance as random embeddings
**Lesson**: Pre-trained embeddings must match target language and script.

### 3. Data Requirements
**Sufficient Data (Q1)**: 
- ~500 Urdu tweets
- Traditional models (BiLSTM) achieve 0.591 F1
- Pre-trained models reach 0.711 F1
**Insufficient Data (Q3/Q4)**:
- ~82 parallel sentences
- All custom models fail with mode collapse
- Only pre-trained mBART succeeds
**Threshold**: Neural MT requires 10,000+ parallel sentences minimum.

### 4. Architecture Comparison
#### For Sentiment Analysis (Q1)
1. Transformer-based (XLM-RoBERTa) > mBERT > BiLSTM > LSTM > GRU > RNN
#### For Translation (Q3)
1. Pre-trained Transformer (mBART) >>> All others (all fail)
**Conclusion**: Architecture matters less than pre-training for low-resource languages.